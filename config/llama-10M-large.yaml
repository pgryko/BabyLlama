data:
  tokenizer_path: "./models/gpt-clean-16000.json"
  train_path: "./data/babylm_10M_clean_1B"
  eval_path: "./data/babylm_dev_clean_1B"
  seq_length: 128
  eval_samples: 2000  # More samples for better evaluation

model:
  type: "Llama"
  name: "Llama-10M-1B"
  hidden_size: 192
  intermediate_size: 768
  n_layer: 6
  n_head: 6
  tie_word_embeddings: False

training:
  lr: 3e-4
  batch_size: 32  # Smaller batch size for 1B tokens
  num_epochs: 1   # Single epoch for 1B tokens
  gradient_accumulation_steps: 8  # Effective batch size = 32 * 8 = 256
  warmup_steps: 2000  # More warmup for larger dataset
  fp16: True
  save_steps: 10000   # Save checkpoints less frequently for large dataset
  eval_steps: 5000    # Evaluate less frequently
  logging_steps: 500  # Less frequent logging for large dataset

logging:
  wandb: False  # Disable wandb for now
  project: "babylm-10M-1M"
  output_dir: "./models/"
  
# Additional settings for large-scale training
optimization:
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
